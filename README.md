# speech-to-text-to-speech-AI
Assistente de voz em Python no Google Colab que grava Ã¡udio no navegador, transcreve com Whisper, gera respostas com modelos da Hugging Face e converte texto em fala em portuguÃªs. Projeto que integra reconhecimento de fala, NLP (Processamento de Linguagem Natura) e sÃ­ntese de voz em um fluxo completo. Esse Ã© o desafio final do Bootcampo Bradesco - GenAI & Dados, sendo alterado para que pudesse ser utilizado com ferramentas gratuitas, incluindo atualizaÃ§Ãµes de cÃ³digo para que seja executÃ¡vel em 2026.

# ğŸ™ï¸ Assistente de Voz com Whisper + Hugging Face no Google Colab

Este projeto cria um **assistente de voz em portuguÃªs** que:
1. Grava Ã¡udio no navegador.
2. Transcreve fala em texto usando **Whisper**.
3. Gera respostas com modelos gratuitos da **Hugging Face**.
4. Converte texto em fala (TTS) para responder em Ã¡udio.
---

ğŸ“Œ ObservaÃ§Ãµes

â€¢ 	Projeto atualizado para ser 100% gratuito, sem necessidade de chave da OpenAI.<br>
â€¢ 	Ideal para iniciantes em IA aplicada Ã  voz com base em Python e JS.<br>
â€¢ 	Pode ser expandido com outros modelos de linguagem ou TTS.

---
## Teste no Google Colab

VocÃª pode abrir e executar este projeto diretamente no Google Colab clicando no botÃ£o abaixo:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_j2vNKUs0WP3W0bGgs_wEddEyfrOT8oO#scrollTo=ZXpYi2m8NncD)

---

## ğŸš€ ConfiguraÃ§Ã£o Inicial
Definimos o idioma global:

```python
language = "pt"

````
ğŸ¤ Etapa 1: GravaÃ§Ã£o de Ãudio
Captura de Ã¡udio via navegador usando JavaScript integrado ao Colab com cÃ³digo atualizado.

```from IPython.display import display, Javascript
from google.colab import output
from base64 import b64decode

Record = """
async function Record(ms) {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const recorder = new MediaRecorder(stream);
  let data = [];
  recorder.ondataavailable = event => data.push(event.data);
  recorder.start();
  await new Promise(resolve => setTimeout(resolve, ms));
  recorder.stop();
  await new Promise(resolve => recorder.onstop = resolve);
  const audioBlob = new Blob(data);
  const reader = new FileReader();
  reader.readAsDataURL(audioBlob);
  return new Promise(resolve => {
    reader.onloadend = () => resolve(reader.result);
  });
}
"""

def recorder(sec=5):
    display(Javascript(Record))
    js_result = output.eval_js(f'Record({sec * 1000})')
    audio = b64decode(js_result.split(',')[1])
    file_name = 'request_audio.wav'
    with open(file_name, "wb") as f:
        f.write(audio)
    return f'/content/{file_name}'

record_file = recorder(5)  #grava 5 segundos

```
ğŸ“ Etapa 2: Reconhecimento de fala (TranscriÃ§Ã£o do Ãudio) usando o modelo Whisper.

```
!pip install -q openai-whisper
import whisper

model = whisper.load_model("small")
result = model.transcribe(record_file, fp16=False, language=language)
transcription = result["text"]

print("TranscriÃ§Ã£o:", transcription)

```
ğŸ¤– Etapa 3: Resposta com Hugging Face, substituindo o ChatGPT (gpt-3.5-turbo ou superior), sem necessidade de uma api_key e gratuito.<br>
Por o Hugging Face usar um modelo antigo (GPT2), podem ocorrer alucinaÃ§Ãµes da IA.

```
!pip install -q transformers torch
from transformers import pipeline

chatbot = pipeline("text-generation", model="pierreguillou/gpt2-small-portuguese")
chat_response = chatbot(transcription, max_length=100, do_sample=True, top_p=0.95, temperature=0.7)[0]["generated_text"]

print("Resposta:", chat_response)

```
ğŸ”Š Etapa 4: ConversÃ£o em Ãudio com Hugging Face TTS. A bibliotega gTTS performa melhor nessa etapa e tambÃ©m Ã© gratuita.
Utilizando o hfTTS apenas para mostrar alternativas.

```
!pip install -q transformers soundfile
from transformers import pipeline
import soundfile as sf
import numpy as np
from IPython.display import Audio

tts = pipeline("text-to-speech", model="facebook/mms-tts-por")
speech = tts(chat_response)

# Ajustar velocidade (ex.: 1.25x mais rÃ¡pido)
speed_factor = 1.25
new_audio = np.interp(
    np.arange(0, len(speech["audio"]), speed_factor),
    np.arange(0, len(speech["audio"])),
    speech["audio"]
)

response_audio_path = "/content/response_audio_fast.wav"
sf.write(response_audio_path, new_audio, speech["sampling_rate"])

Audio(response_audio_path, autoplay=True)

```
âœ… Fluxo Completo
1. 	ğŸ¤ VocÃª fala â†’ Ã¡udio Ã© gravado.
2. 	ğŸ“ Whisper transcreve para texto.
3. 	ğŸ¤– Hugging Face gera resposta.
4. 	ğŸ”Š Resposta Ã© falada de volta para vocÃª.
